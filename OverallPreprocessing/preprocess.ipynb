{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, expr, count, when, log1p, lit, year, month, dayofmonth, datediff, mean, first, to_timestamp\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"FYPPreprocessing\") \\\n",
    "#     .config(\"spark.driver.memory\", \"8g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"16g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FYPPreprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./combined.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------------+-----------+--------------------+-------------------+-------------------+-----------------+-----------------+--------------+---+------------------+--------------------+----------+--------------+----------------+----------------------+----------+--------------+---------------+-------+----------+---------+----------+------+----------+------+----------+------+----------+------+----------+------+----------+------+--------------------+------+-------------------+-------+-----------+--------------------+--------------------+----------------+-------+----------------+---------+\n",
      "|source_id|source_name|source_type|iso3_country|     sector|           subsector|         start_time|           end_time|              lat|              lon|  geometry_ref|gas|emissions_quantity|temporal_granularity|  activity|activity_units|emissions_factor|emissions_factor_units|  capacity|capacity_units|capacity_factor| other1|other1_def|   other2|other2_def|other3|other3_def|other4|other4_def|other5|other5_def|other6|other6_def|other7|other7_def|other8|          other8_def|other9|         other9_def|other10|other10_def|        created_date|       modified_date|reporting_entity|lat_lon|native_source_id|sector_id|\n",
      "+---------+-----------+-----------+------------+-----------+--------------------+-------------------+-------------------+-----------------+-----------------+--------------+---+------------------+--------------------+----------+--------------+----------------+----------------------+----------+--------------+---------------+-------+----------+---------+----------+------+----------+------+----------+------+----------+------+----------+------+----------+------+--------------------+------+-------------------+-------+-----------+--------------------+--------------------+----------------+-------+----------------+---------+\n",
      "| 21094711|    Ab Band|       NULL|         AFG|agriculture|enteric-fermentat...|2021-01-01 00:00:00|2021-01-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1| bc|               0.0|               month|2081.47216|animal head(s)|             0.0|  t of BC per anima...|2081.47216|animal head(s)|            1.0|AFG.9_1|      NULL|AFG.9.1_1|      NULL|     2|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|FUA_Area (square_km)|  NULL|FUA_population_2015|   NULL|       NULL|2024-10-28 21:15:...|2024-10-28 21:15:...|   climate-trace|   NULL|       AFG.9.1_1|     NULL|\n",
      "| 21094711|    Ab Band|       NULL|         AFG|agriculture|enteric-fermentat...|2021-02-01 00:00:00|2021-02-28 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1| bc|               0.0|               month|2081.47216|animal head(s)|             0.0|  t of BC per anima...|2081.47216|animal head(s)|            1.0|AFG.9_1|      NULL|AFG.9.1_1|      NULL|     2|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|FUA_Area (square_km)|  NULL|FUA_population_2015|   NULL|       NULL|2024-10-28 21:15:...|2024-10-28 21:15:...|   climate-trace|   NULL|       AFG.9.1_1|     NULL|\n",
      "| 21094711|    Ab Band|       NULL|         AFG|agriculture|enteric-fermentat...|2021-03-01 00:00:00|2021-03-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1| bc|               0.0|               month|2081.47216|animal head(s)|             0.0|  t of BC per anima...|2081.47216|animal head(s)|            1.0|AFG.9_1|      NULL|AFG.9.1_1|      NULL|     2|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|FUA_Area (square_km)|  NULL|FUA_population_2015|   NULL|       NULL|2024-10-28 21:15:...|2024-10-28 21:15:...|   climate-trace|   NULL|       AFG.9.1_1|     NULL|\n",
      "| 21094711|    Ab Band|       NULL|         AFG|agriculture|enteric-fermentat...|2021-04-01 00:00:00|2021-04-30 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1| bc|               0.0|               month|2081.47216|animal head(s)|             0.0|  t of BC per anima...|2081.47216|animal head(s)|            1.0|AFG.9_1|      NULL|AFG.9.1_1|      NULL|     2|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|FUA_Area (square_km)|  NULL|FUA_population_2015|   NULL|       NULL|2024-10-28 21:15:...|2024-10-28 21:15:...|   climate-trace|   NULL|       AFG.9.1_1|     NULL|\n",
      "| 21094711|    Ab Band|       NULL|         AFG|agriculture|enteric-fermentat...|2021-05-01 00:00:00|2021-05-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1| bc|               0.0|               month|2081.47216|animal head(s)|             0.0|  t of BC per anima...|2081.47216|animal head(s)|            1.0|AFG.9_1|      NULL|AFG.9.1_1|      NULL|     2|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|      NULL|  NULL|FUA_Area (square_km)|  NULL|FUA_population_2015|   NULL|       NULL|2024-10-28 21:15:...|2024-10-28 21:15:...|   climate-trace|   NULL|       AFG.9.1_1|     NULL|\n",
      "+---------+-----------+-----------+------------+-----------+--------------------+-------------------+-------------------+-----------------+-----------------+--------------+---+------------------+--------------------+----------+--------------+----------------+----------------------+----------+--------------+---------------+-------+----------+---------+----------+------+----------+------+----------+------+----------+------+----------+------+----------+------+--------------------+------+-------------------+-------+-----------+--------------------+--------------------+----------------+-------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- source_name: string (nullable = true)\n",
      " |-- source_type: string (nullable = true)\n",
      " |-- iso3_country: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- subsector: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      " |-- geometry_ref: string (nullable = true)\n",
      " |-- gas: string (nullable = true)\n",
      " |-- emissions_quantity: string (nullable = true)\n",
      " |-- temporal_granularity: string (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- activity_units: string (nullable = true)\n",
      " |-- emissions_factor: string (nullable = true)\n",
      " |-- emissions_factor_units: string (nullable = true)\n",
      " |-- capacity: string (nullable = true)\n",
      " |-- capacity_units: string (nullable = true)\n",
      " |-- capacity_factor: double (nullable = true)\n",
      " |-- other1: string (nullable = true)\n",
      " |-- other1_def: string (nullable = true)\n",
      " |-- other2: string (nullable = true)\n",
      " |-- other2_def: string (nullable = true)\n",
      " |-- other3: string (nullable = true)\n",
      " |-- other3_def: string (nullable = true)\n",
      " |-- other4: string (nullable = true)\n",
      " |-- other4_def: string (nullable = true)\n",
      " |-- other5: string (nullable = true)\n",
      " |-- other5_def: string (nullable = true)\n",
      " |-- other6: string (nullable = true)\n",
      " |-- other6_def: string (nullable = true)\n",
      " |-- other7: string (nullable = true)\n",
      " |-- other7_def: string (nullable = true)\n",
      " |-- other8: string (nullable = true)\n",
      " |-- other8_def: string (nullable = true)\n",
      " |-- other9: string (nullable = true)\n",
      " |-- other9_def: string (nullable = true)\n",
      " |-- other10: string (nullable = true)\n",
      " |-- other10_def: string (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      " |-- reporting_entity: string (nullable = true)\n",
      " |-- lat_lon: string (nullable = true)\n",
      " |-- native_source_id: string (nullable = true)\n",
      " |-- sector_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show column data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the 'other' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns to remove\n",
    "columns_to_remove = [f\"other{i}\" for i in range(1, 11)] + [f\"other{i}_def\" for i in range(1, 11)]\n",
    "\n",
    "# Drop the columns from the DataFrame (PySpark equivalent of 'errors=\"ignore\"')\n",
    "existing_columns_to_remove = [col for col in columns_to_remove if col in df.columns]\n",
    "\n",
    "df_cleaned = df.drop(*existing_columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing other useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\"source_type\", \"sector_id\", \"lat_lon\", \"temporal_granularity\", \"reporting_entity\", \"native_source_id\"]\n",
    "\n",
    "# Drop the columns (handling missing ones safely)\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df_cleaned.columns]\n",
    "\n",
    "df_cleaned = df_cleaned.drop(*existing_columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling empty useless rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id: 0 missing values\n",
      "source_name: 59616 missing values\n",
      "iso3_country: 0 missing values\n",
      "sector: 0 missing values\n",
      "subsector: 0 missing values\n",
      "start_time: 0 missing values\n",
      "end_time: 0 missing values\n",
      "lat: 7663152 missing values\n",
      "lon: 7663152 missing values\n",
      "geometry_ref: 0 missing values\n",
      "gas: 0 missing values\n",
      "emissions_quantity: 0 missing values\n",
      "activity: 6048 missing values\n",
      "activity_units: 0 missing values\n",
      "emissions_factor: 6048 missing values\n",
      "emissions_factor_units: 0 missing values\n",
      "capacity: 6048 missing values\n",
      "capacity_units: 0 missing values\n",
      "capacity_factor: 9888 missing values\n",
      "created_date: 864 missing values\n",
      "modified_date: 3024 missing values\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column\n",
    "for column in df_cleaned.columns:\n",
    "    missing_count = df_cleaned.select(count(when(col(column).isNull(), column)).alias(\"missing_count\")).collect()[0][\"missing_count\"]\n",
    "    print(f\"{column}: {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where iso3_country is NULL\n",
    "df_cleaned = df_cleaned.filter(col(\"iso3_country\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typecasting the integers and floats to the correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns to convert\n",
    "numeric_columns = [\"source_id\", \"lat\", \"lon\", \"emissions_quantity\", \"activity\", \n",
    "                   \"emissions_factor\", \"capacity\", \"capacity_factor\"]\n",
    "\n",
    "# Convert each column to the correct type\n",
    "for column in numeric_columns:\n",
    "    dtype = \"int\" if column == \"source_id\" else \"double\"  # Convert source_id to int, others to double\n",
    "    df_cleaned = df_cleaned.withColumn(column, col(column).cast(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: integer (nullable = true)\n",
      " |-- source_name: string (nullable = true)\n",
      " |-- iso3_country: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      " |-- subsector: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- geometry_ref: string (nullable = true)\n",
      " |-- gas: string (nullable = true)\n",
      " |-- emissions_quantity: double (nullable = true)\n",
      " |-- activity: double (nullable = true)\n",
      " |-- activity_units: string (nullable = true)\n",
      " |-- emissions_factor: double (nullable = true)\n",
      " |-- emissions_factor_units: string (nullable = true)\n",
      " |-- capacity: double (nullable = true)\n",
      " |-- capacity_units: string (nullable = true)\n",
      " |-- capacity_factor: double (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the schema\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat: Found 16800 outliers\n",
      "lon: Found 864 outliers\n",
      "activity: Found 2619538 outliers\n",
      "emissions_factor: Found 4744879 outliers\n",
      "capacity: Found 4095511 outliers\n",
      "capacity_factor: Found 3234216 outliers\n"
     ]
    }
   ],
   "source": [
    "# Define columns to check for outliers\n",
    "columns = ['lat', 'lon', 'activity', 'emissions_factor', 'capacity', 'capacity_factor']\n",
    "\n",
    "# Compute IQR and detect outliers\n",
    "for column in columns:\n",
    "    # Compute Q1 and Q3\n",
    "    Q1 = df_cleaned.approxQuantile(column, [0.25], 0.01)[0]  # Approximate quantile for performance\n",
    "    Q3 = df_cleaned.approxQuantile(column, [0.75], 0.01)[0]\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier conditions\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter outliers\n",
    "    outliers = df_cleaned.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "    \n",
    "    # Print outlier count\n",
    "    print(f\"{column}: Found {outliers.count()} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for 'lat' and 'lon' using MEAN\n",
    "lat_mean = df_cleaned.select(mean(col(\"lat\"))).collect()[0][0]\n",
    "df_cleaned = df_cleaned.fillna({\"lat\": lat_mean})\n",
    "\n",
    "lon_mean = df_cleaned.select(mean(col(\"lon\"))).collect()[0][0]\n",
    "df_cleaned = df_cleaned.fillna({\"lon\": lon_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for other columns using MEDIAN\n",
    "def get_median(df, column):\n",
    "    return df.approxQuantile(column, [0.5], 0.01)[0]  # Approximate median\n",
    "\n",
    "columns_to_fill = [\"activity\", \"emissions_factor\", \"capacity\", \"capacity_factor\"]\n",
    "\n",
    "for col_name in columns_to_fill:\n",
    "    median_value = get_median(df_cleaned, col_name)\n",
    "    df_cleaned = df_cleaned.fillna({col_name: median_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso3_country: 0 missing values\n",
      "sector: 0 missing values\n",
      "subsector: 0 missing values\n",
      "source_id: 0 missing values\n",
      "start_time: 0 missing values\n",
      "end_time: 0 missing values\n",
      "lat: 0 missing values\n",
      "lon: 0 missing values\n",
      "geometry_ref: 0 missing values\n",
      "gas: 0 missing values\n",
      "emissions_quantity: 0 missing values\n",
      "activity: 0 missing values\n",
      "activity_units: 0 missing values\n",
      "emissions_factor: 0 missing values\n",
      "emissions_factor_units: 0 missing values\n",
      "capacity: 0 missing values\n",
      "capacity_units: 0 missing values\n",
      "capacity_factor: 0 missing values\n",
      "created_date: 0 missing values\n",
      "modified_date: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each column\n",
    "for column in df_cleaned.columns:\n",
    "    missing_count = df_cleaned.select(count(when(col(column).isNull(), column)).alias(\"missing_count\")).collect()[0][\"missing_count\"]\n",
    "    print(f\"{column}: {missing_count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where emissions_quantity is NULL\n",
    "df_cleaned = df_cleaned.filter(col(\"emissions_quantity\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the missing values of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the most frequent source_name per group\n",
    "mode_df = (\n",
    "    df_cleaned.groupBy(\"iso3_country\", \"sector\", \"subsector\", \"source_name\")\n",
    "    .agg(count(\"*\").alias(\"count\"))  # Count occurrences\n",
    "    .withColumn(\"rank\", count(\"*\").over(Window.partitionBy(\"iso3_country\", \"sector\", \"subsector\").orderBy(col(\"count\").desc())))\n",
    "    .filter(col(\"rank\") == 1)  # Keep only the most frequent value\n",
    "    .drop(\"count\", \"rank\")  # Drop extra columns\n",
    ")\n",
    "\n",
    "# Step 2: Join back to the original DataFrame to fill missing values\n",
    "df_cleaned = (\n",
    "    df_cleaned.alias(\"df1\")\n",
    "    .join(mode_df.alias(\"df2\"), on=[\"iso3_country\", \"sector\", \"subsector\"], how=\"left\")\n",
    "    .withColumn(\n",
    "        \"source_name\",\n",
    "        when(col(\"df1.source_name\").isNull(), col(\"df2.source_name\")).otherwise(col(\"df1.source_name\"))\n",
    "    )\n",
    "    .select(\"df1.*\")  # Keep original structure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capping Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the distribution of the columns that had outliers, to determine the method for capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 551.0 failed 1 times, most recent failure: Lost task 0.0 in stage 551.0 (TID 12518) (192.168.0.106 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(bins))  \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Compute histogram counts\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mdf_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract bin edges and counts\u001b[39;00m\n\u001b[0;32m     14\u001b[0m bin_edges, counts \u001b[38;5;241m=\u001b[39m hist[\u001b[38;5;241m0\u001b[39m], hist[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2499\u001b[0m, in \u001b[0;36mRDD.histogram\u001b[1;34m(self, buckets)\u001b[0m\n\u001b[0;32m   2496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmergeCounters\u001b[39m(a: List[\u001b[38;5;28mint\u001b[39m], b: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   2497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;241m+\u001b[39m j \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, b)]\n\u001b[1;32m-> 2499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buckets, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistogram\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmergeCounters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[1;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 551.0 failed 1 times, most recent failure: Lost task 0.0 in stage 551.0 (TID 12518) (192.168.0.106 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:713)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:757)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "columns = ['activity', 'emissions_factor', 'capacity', 'capacity_factor']\n",
    "\n",
    "for col in columns:\n",
    "    # Get histogram bins using approximate quantiles\n",
    "    bins = df_cleaned.approxQuantile(col, [i / 30 for i in range(31)], 0.05)\n",
    "\n",
    "    # Remove duplicate values (PySpark histogram needs unique bins)\n",
    "    bins = sorted(set(bins))  \n",
    "\n",
    "    # Compute histogram counts\n",
    "    hist = df_cleaned.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "\n",
    "    # Extract bin edges and counts\n",
    "    bin_edges, counts = hist[0], hist[1]\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.bar(bin_edges[:-1], counts, width=(bin_edges[1] - bin_edges[0]), alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will be using log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"activity\", log1p(\"activity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"emissions_factor\",\n",
    "    log1p(col(\"emissions_factor\").cast(\"double\") + lit(1e-6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"capacity\", log1p(col(\"capacity\").cast(\"double\") + lit(1e-6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"capacity_factor\", log1p(col(\"capacity_factor\").cast(\"double\") + lit(1e-6))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the data types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sector: string (nullable = true)\n",
      " |-- subsector: string (nullable = true)\n",
      " |-- source_id: integer (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- lat: double (nullable = false)\n",
      " |-- lon: double (nullable = false)\n",
      " |-- geometry_ref: string (nullable = true)\n",
      " |-- gas: string (nullable = true)\n",
      " |-- emissions_quantity: double (nullable = true)\n",
      " |-- activity: double (nullable = true)\n",
      " |-- activity_units: string (nullable = true)\n",
      " |-- emissions_factor: double (nullable = true)\n",
      " |-- emissions_factor_units: string (nullable = true)\n",
      " |-- capacity: double (nullable = true)\n",
      " |-- capacity_units: string (nullable = true)\n",
      " |-- capacity_factor: double (nullable = true)\n",
      " |-- created_date: timestamp (nullable = true)\n",
      " |-- modified_date: timestamp (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- start_month: integer (nullable = true)\n",
      " |-- start_day: integer (nullable = true)\n",
      " |-- end_year: integer (nullable = true)\n",
      " |-- end_month: integer (nullable = true)\n",
      " |-- end_day: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- iso3_country_ohe_0: double (nullable = true)\n",
      " |-- iso3_country_ohe_1: double (nullable = true)\n",
      " |-- iso3_country_ohe_2: double (nullable = true)\n",
      " |-- iso3_country_ohe_3: double (nullable = true)\n",
      " |-- iso3_country_ohe_4: double (nullable = true)\n",
      " |-- iso3_country_ohe_5: double (nullable = true)\n",
      " |-- iso3_country_ohe_6: double (nullable = true)\n",
      " |-- iso3_country_ohe_7: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"start_time\", to_timestamp(\"start_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"end_time\", to_timestamp(\"end_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"created_date\", to_timestamp(\"created_date\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"modified_date\", to_timestamp(\"modified_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature creation based on recording dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting start year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"start_year\", year(\"start_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"start_month\", month(\"start_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"start_day\", dayofmonth(\"start_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting end year, month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"end_year\", year(\"end_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"end_month\", month(\"end_time\"))\n",
    "df_cleaned = df_cleaned.withColumn(\"end_day\", dayofmonth(\"end_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new feature, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.withColumn(\"duration\", datediff(\"end_time\", \"start_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE-ing the country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert categorical column into numerical indices\n",
    "indexer = StringIndexer(inputCol=\"iso3_country\", outputCol=\"iso3_country_index\", handleInvalid=\"keep\")\n",
    "indexer_model = indexer.fit(df_cleaned)\n",
    "df_cleaned = indexer_model.transform(df_cleaned)\n",
    "\n",
    "# Get category labels (ordered by index)\n",
    "category_names = indexer_model.labels\n",
    "\n",
    "# Step 2: Apply OneHotEncoding to the indexed column\n",
    "encoder = OneHotEncoder(inputCol=\"iso3_country_index\", outputCol=\"iso3_country_ohe\")\n",
    "df_cleaned = encoder.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "# Step 3: Convert the vector column into an array\n",
    "df_cleaned = df_cleaned.withColumn(\"iso3_country_ohe_array\", vector_to_array(\"iso3_country_ohe\"))\n",
    "\n",
    "# Step 4: Split array into separate columns with category names\n",
    "for i, category in enumerate(category_names):\n",
    "    df_cleaned = df_cleaned.withColumn(f\"iso3_country_{category}\", col(\"iso3_country_ohe_array\")[i])\n",
    "\n",
    "# Step 5: Drop the unnecessary columns\n",
    "df_cleaned = df_cleaned.drop(\"iso3_country\", \"iso3_country_index\", \"iso3_country_ohe\", \"iso3_country_ohe_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE-ing the sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert categorical column into numerical indices\n",
    "indexer = StringIndexer(inputCol=\"sector\", outputCol=\"sector_index\", handleInvalid=\"keep\")\n",
    "indexer_model = indexer.fit(df_cleaned)\n",
    "df_cleaned = indexer_model.transform(df_cleaned)\n",
    "\n",
    "# Get category labels (ordered by index)\n",
    "category_names = indexer_model.labels\n",
    "\n",
    "# Step 2: Apply OneHotEncoding to the indexed column\n",
    "encoder = OneHotEncoder(inputCol=\"sector_index\", outputCol=\"sector_ohe\")\n",
    "df_cleaned = encoder.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "# Step 3: Convert the vector column into an array\n",
    "df_cleaned = df_cleaned.withColumn(\"sector_ohe_array\", vector_to_array(\"sector_ohe\"))\n",
    "\n",
    "# Step 4: Split array into separate columns with category names\n",
    "for i, category in enumerate(category_names):\n",
    "    df_cleaned = df_cleaned.withColumn(f\"sector_{category}\", col(\"sector_ohe_array\")[i])\n",
    "\n",
    "# Step 5: Drop the unnecessary columns\n",
    "df_cleaned = df_cleaned.drop(\"sector\", \"sector_index\", \"sector_ohe\", \"sector_ohe_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE-ing the sub-sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert categorical column into numerical indices\n",
    "indexer = StringIndexer(inputCol=\"subsector\", outputCol=\"subsector_index\", handleInvalid=\"keep\")\n",
    "indexer_model = indexer.fit(df_cleaned)\n",
    "df_cleaned = indexer_model.transform(df_cleaned)\n",
    "\n",
    "# Get category labels (ordered by index)\n",
    "category_names = indexer_model.labels\n",
    "\n",
    "# Step 2: Apply OneHotEncoding to the indexed column\n",
    "encoder = OneHotEncoder(inputCol=\"subsector_index\", outputCol=\"subsector_ohe\")\n",
    "df_cleaned = encoder.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "# Step 3: Convert the vector column into an array\n",
    "df_cleaned = df_cleaned.withColumn(\"subsector_ohe_array\", vector_to_array(\"subsector_ohe\"))\n",
    "\n",
    "# Step 4: Split array into separate columns with category names\n",
    "for i, category in enumerate(category_names):\n",
    "    df_cleaned = df_cleaned.withColumn(f\"subsector_{category}\", col(\"subsector_ohe_array\")[i])\n",
    "\n",
    "# Step 5: Drop the unnecessary columns\n",
    "df_cleaned = df_cleaned.drop(\"subsector\", \"subsector_index\", \"subsector_ohe\", \"subsector_ohe_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE-ing the gas column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert categorical column into numerical indices\n",
    "indexer = StringIndexer(inputCol=\"gas\", outputCol=\"gas_index\", handleInvalid=\"keep\")\n",
    "indexer_model = indexer.fit(df_cleaned)\n",
    "df_cleaned = indexer_model.transform(df_cleaned)\n",
    "\n",
    "# Get category labels (ordered by index)\n",
    "category_names = indexer_model.labels\n",
    "\n",
    "# Step 2: Apply OneHotEncoding to the indexed column\n",
    "encoder = OneHotEncoder(inputCol=\"gas_index\", outputCol=\"gas_ohe\")\n",
    "df_cleaned = encoder.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "# Step 3: Convert the vector column into an array\n",
    "df_cleaned = df_cleaned.withColumn(\"gas_ohe_array\", vector_to_array(\"gas_ohe\"))\n",
    "\n",
    "# Step 4: Split array into separate columns with category names\n",
    "for i, category in enumerate(category_names):\n",
    "    df_cleaned = df_cleaned.withColumn(f\"gas_{category}\", col(\"gas_ohe_array\")[i])\n",
    "\n",
    "# Step 5: Drop the unnecessary columns\n",
    "df_cleaned = df_cleaned.drop(\"gas\", \"gas_index\", \"gas_ohe\", \"gas_ohe_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-----------------+-----------------+--------------+------------------+--------+--------------+--------------------+----------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+-----------+---------+--------+---------+-------+--------+------------------+----------------------------+----------------+---------------------+--------------------+------------+------------+-----------------------------+-------------------------+--------------------------+-----------------------------+----------------------------------+------------------------------------------+---------------------------------------+---------------------------------------------+---------------------------------------+-------------------------------------------+------------------+------------------------+------------------------+-------------------------+---------------------+------------------------------+--------------------------+---------------------------------+---------------------------+-----------------------+--------------------------------+-----------------------------------------------------+------------------------------+---------------------+---------------------------+--------------------------------+-----------------------------------------------+--------------------------------------------+----------------+--------------------------+---------------------------+--------------------------------+-------------------------------------------------------+---------------------+-------------------+------------------------+------------------------+------------------------------+------------------+--------------------------------------+--------------------------------+-------------------------------+------------------------+-----------------------+--------------+-------+-------+-------------+-------+-------+------+------+---------+\n",
      "|source_id|         start_time|           end_time|              lat|              lon|  geometry_ref|emissions_quantity|activity|activity_units|    emissions_factor|emissions_factor_units|            capacity|capacity_units|     capacity_factor|        created_date|       modified_date|start_year|start_month|start_day|end_year|end_month|end_day|duration|sector_agriculture|sector_forestry-and-land-use|sector_buildings|sector_transportation|sector_manufacturing|sector_waste|sector_power|sector_fossil-fuel-operations|sector_mineral-extraction|subsector_rice-cultivation|subsector_road-transportation|subsector_textiles-leather-apparel|subsector_synthetic-fertilizer-application|subsector_residential-onsite-fuel-usage|subsector_enteric-fermentation-cattle-pasture|subsector_manure-left-on-pasture-cattle|subsector_non-residential-onsite-fuel-usage|subsector_removals|subsector_cropland-fires|subsector_net-shrubgrass|subsector_net-forest-land|subsector_net-wetland|subsector_forest-land-clearing|subsector_shrubgrass-fires|subsector_forest-land-degradation|subsector_forest-land-fires|subsector_wetland-fires|subsector_electricity-generation|subsector_domestic-wastewater-treatment-and-discharge|subsector_solid-waste-disposal|subsector_coal-mining|subsector_domestic-aviation|subsector_international-aviation|subsector_enteric-fermentation-cattle-operation|subsector_manure-management-cattle-operation|subsector_cement|subsector_water-reservoirs|subsector_domestic-shipping|subsector_international-shipping|subsector_industrial-wastewater-treatment-and-discharge|subsector_iron-mining|subsector_chemicals|subsector_bauxite-mining|subsector_iron-and-steel|subsector_oil-and-gas-refining|subsector_aluminum|subsector_petrochemical-steam-cracking|subsector_oil-and-gas-production|subsector_oil-and-gas-transport|subsector_pulp-and-paper|subsector_copper-mining|gas_co2e_100yr|gas_ch4|gas_co2|gas_co2e_20yr|gas_nox|gas_so2|gas_co|gas_bc|gas_pm2_5|\n",
      "+---------+-------------------+-------------------+-----------------+-----------------+--------------+------------------+--------+--------------+--------------------+----------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+-----------+---------+--------+---------+-------+--------+------------------+----------------------------+----------------+---------------------+--------------------+------------+------------+-----------------------------+-------------------------+--------------------------+-----------------------------+----------------------------------+------------------------------------------+---------------------------------------+---------------------------------------------+---------------------------------------+-------------------------------------------+------------------+------------------------+------------------------+-------------------------+---------------------+------------------------------+--------------------------+---------------------------------+---------------------------+-----------------------+--------------------------------+-----------------------------------------------------+------------------------------+---------------------+---------------------------+--------------------------------+-----------------------------------------------+--------------------------------------------+----------------+--------------------------+---------------------------+--------------------------------+-------------------------------------------------------+---------------------+-------------------+------------------------+------------------------+------------------------------+------------------+--------------------------------------+--------------------------------+-------------------------------+------------------------+-----------------------+--------------+-------+-------+-------------+-------+-------+------+------+---------+\n",
      "|  1014004|2021-01-01 00:00:00|2021-01-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1|               0.0|     0.0|            MJ|9.999995000003334E-7|        t of BC per MJ|9.999995000003334E-7|            m2|9.999995000003334E-7|2024-10-28 21:15:...|2024-10-28 21:15:...|      2021|          1|        1|    2021|        1|     31|      30|               0.0|                         0.0|             1.0|                  0.0|                 0.0|         0.0|         0.0|                          0.0|                      0.0|                       0.0|                          0.0|                               0.0|                                       0.0|                                    0.0|                                          0.0|                                    0.0|                                        1.0|               0.0|                     0.0|                     0.0|                      0.0|                  0.0|                           0.0|                       0.0|                              0.0|                        0.0|                    0.0|                             0.0|                                                  0.0|                           0.0|                  0.0|                        0.0|                             0.0|                                            0.0|                                         0.0|             0.0|                       0.0|                        0.0|                             0.0|                                                    0.0|                  0.0|                0.0|                     0.0|                     0.0|                           0.0|               0.0|                                   0.0|                             0.0|                            0.0|                     0.0|                    0.0|           0.0|    0.0|    0.0|          0.0|    0.0|    0.0|   0.0|   1.0|      0.0|\n",
      "|  1014004|2021-02-01 00:00:00|2021-02-28 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1|               0.0|     0.0|            MJ|9.999995000003334E-7|        t of BC per MJ|9.999995000003334E-7|            m2|9.999995000003334E-7|2024-10-28 21:15:...|2024-10-28 21:15:...|      2021|          2|        1|    2021|        2|     28|      27|               0.0|                         0.0|             1.0|                  0.0|                 0.0|         0.0|         0.0|                          0.0|                      0.0|                       0.0|                          0.0|                               0.0|                                       0.0|                                    0.0|                                          0.0|                                    0.0|                                        1.0|               0.0|                     0.0|                     0.0|                      0.0|                  0.0|                           0.0|                       0.0|                              0.0|                        0.0|                    0.0|                             0.0|                                                  0.0|                           0.0|                  0.0|                        0.0|                             0.0|                                            0.0|                                         0.0|             0.0|                       0.0|                        0.0|                             0.0|                                                    0.0|                  0.0|                0.0|                     0.0|                     0.0|                           0.0|               0.0|                                   0.0|                             0.0|                            0.0|                     0.0|                    0.0|           0.0|    0.0|    0.0|          0.0|    0.0|    0.0|   0.0|   1.0|      0.0|\n",
      "|  1014004|2021-03-01 00:00:00|2021-03-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1|               0.0|     0.0|            MJ|9.999995000003334E-7|        t of BC per MJ|9.999995000003334E-7|            m2|9.999995000003334E-7|2024-10-28 21:15:...|2024-10-28 21:15:...|      2021|          3|        1|    2021|        3|     31|      30|               0.0|                         0.0|             1.0|                  0.0|                 0.0|         0.0|         0.0|                          0.0|                      0.0|                       0.0|                          0.0|                               0.0|                                       0.0|                                    0.0|                                          0.0|                                    0.0|                                        1.0|               0.0|                     0.0|                     0.0|                      0.0|                  0.0|                           0.0|                       0.0|                              0.0|                        0.0|                    0.0|                             0.0|                                                  0.0|                           0.0|                  0.0|                        0.0|                             0.0|                                            0.0|                                         0.0|             0.0|                       0.0|                        0.0|                             0.0|                                                    0.0|                  0.0|                0.0|                     0.0|                     0.0|                           0.0|               0.0|                                   0.0|                             0.0|                            0.0|                     0.0|                    0.0|           0.0|    0.0|    0.0|          0.0|    0.0|    0.0|   0.0|   1.0|      0.0|\n",
      "|  1014004|2021-04-01 00:00:00|2021-04-30 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1|               0.0|     0.0|            MJ|9.999995000003334E-7|        t of BC per MJ|9.999995000003334E-7|            m2|9.999995000003334E-7|2024-10-28 21:15:...|2024-10-28 21:15:...|      2021|          4|        1|    2021|        4|     30|      29|               0.0|                         0.0|             1.0|                  0.0|                 0.0|         0.0|         0.0|                          0.0|                      0.0|                       0.0|                          0.0|                               0.0|                                       0.0|                                    0.0|                                          0.0|                                    0.0|                                        1.0|               0.0|                     0.0|                     0.0|                      0.0|                  0.0|                           0.0|                       0.0|                              0.0|                        0.0|                    0.0|                             0.0|                                                  0.0|                           0.0|                  0.0|                        0.0|                             0.0|                                            0.0|                                         0.0|             0.0|                       0.0|                        0.0|                             0.0|                                                    0.0|                  0.0|                0.0|                     0.0|                     0.0|                           0.0|               0.0|                                   0.0|                             0.0|                            0.0|                     0.0|                    0.0|           0.0|    0.0|    0.0|          0.0|    0.0|    0.0|   0.0|   1.0|      0.0|\n",
      "|  1014004|2021-05-01 00:00:00|2021-05-31 00:00:00|32.89894286254679|68.04743133652431|gadm_AFG.9.1_1|               0.0|     0.0|            MJ|9.999995000003334E-7|        t of BC per MJ|9.999995000003334E-7|            m2|9.999995000003334E-7|2024-10-28 21:15:...|2024-10-28 21:15:...|      2021|          5|        1|    2021|        5|     31|      30|               0.0|                         0.0|             1.0|                  0.0|                 0.0|         0.0|         0.0|                          0.0|                      0.0|                       0.0|                          0.0|                               0.0|                                       0.0|                                    0.0|                                          0.0|                                    0.0|                                        1.0|               0.0|                     0.0|                     0.0|                      0.0|                  0.0|                           0.0|                       0.0|                              0.0|                        0.0|                    0.0|                             0.0|                                                  0.0|                           0.0|                  0.0|                        0.0|                             0.0|                                            0.0|                                         0.0|             0.0|                       0.0|                        0.0|                             0.0|                                                    0.0|                  0.0|                0.0|                     0.0|                     0.0|                           0.0|               0.0|                                   0.0|                             0.0|                            0.0|                     0.0|                    0.0|           0.0|    0.0|    0.0|          0.0|    0.0|    0.0|   0.0|   1.0|      0.0|\n",
      "+---------+-------------------+-------------------+-----------------+-----------------+--------------+------------------+--------+--------------+--------------------+----------------------+--------------------+--------------+--------------------+--------------------+--------------------+----------+-----------+---------+--------+---------+-------+--------+------------------+----------------------------+----------------+---------------------+--------------------+------------+------------+-----------------------------+-------------------------+--------------------------+-----------------------------+----------------------------------+------------------------------------------+---------------------------------------+---------------------------------------------+---------------------------------------+-------------------------------------------+------------------+------------------------+------------------------+-------------------------+---------------------+------------------------------+--------------------------+---------------------------------+---------------------------+-----------------------+--------------------------------+-----------------------------------------------------+------------------------------+---------------------+---------------------------+--------------------------------+-----------------------------------------------+--------------------------------------------+----------------+--------------------------+---------------------------+--------------------------------+-------------------------------------------------------+---------------------+-------------------+------------------------+------------------------+------------------------------+------------------+--------------------------------------+--------------------------------+-------------------------------+------------------------+-----------------------+--------------+-------+-------+-------------+-------+-------+------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Hadoop NativeIO on Windows to prevent file permission errors while writing Parquet files\n",
    "spark.conf.set(\"spark.hadoop.io.native.lib.available\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2175.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/preprocessed.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2175.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.write.mode(\"overwrite\").parquet(\"./data/preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
